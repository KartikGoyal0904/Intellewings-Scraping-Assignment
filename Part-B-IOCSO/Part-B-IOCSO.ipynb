{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiIgpiLSw2Nb",
        "outputId": "0b04e89f-4c31-4588-dea6-73a4286c799e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "BASE_URL = \"https://www.iosco.org/i-scan/\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "SUMMARY_FIELDS = [\n",
        "    \"Name_company\", \"Corporate Name\", \"site_url\", \"Authority_Name\",\n",
        "    \"Authority_link\", \"date\", \"id\"\n",
        "]\n",
        "\n",
        "DETAIL_FIELDS = [\n",
        "    \"Warning ID\", \"NCA\", \"Date Published At NCA\", \"Date Published At IOSCO\",\n",
        "    \"Last Updated\", \"NCA URL\", \"Commercial Name\", \"URL\", \"Other URL\",\n",
        "    \"Category\", \"Additional Information\"\n",
        "]\n",
        "\n",
        "CHECKPOINT_FILE = \"checkpoint.txt\"\n",
        "\n",
        "def save_checkpoint(page):\n",
        "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
        "        f.write(str(page))\n",
        "\n",
        "def load_checkpoint():\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
        "            return int(f.read().strip())\n",
        "    return 1  # Start from page 1 if no checkpoint\n",
        "\n",
        "def extract_detail_page(warning_id, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            url = f\"{BASE_URL}?id={warning_id}\"\n",
        "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                raise ValueError(f\"HTTP {response.status_code}\")\n",
        "\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            table_div = soup.find(\"div\", class_=\"table-responsive\")\n",
        "            if not table_div:\n",
        "                raise ValueError(\"No table-responsive div\")\n",
        "\n",
        "            table = table_div.find(\"table\")\n",
        "            if not table:\n",
        "                raise ValueError(\"No table found\")\n",
        "\n",
        "            raw_data = {}\n",
        "            for row in table.find_all(\"tr\"):\n",
        "                th = row.find(\"th\")\n",
        "                td = row.find(\"td\")\n",
        "                if not th or not td:\n",
        "                    continue\n",
        "                key = th.get_text(strip=True).replace(\":\", \"\")\n",
        "                a = td.find(\"a\")\n",
        "                value = a[\"href\"] if a else td.get_text(strip=True)\n",
        "                raw_data[key] = value\n",
        "\n",
        "            mapped_data = {key: raw_data.get(key, None) for key in DETAIL_FIELDS}\n",
        "            return mapped_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[Retry {attempt + 1}] Detail fetch failed for ID {warning_id}: {e}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"[ERROR] Final failure for ID {warning_id}\")\n",
        "    return {key: None for key in DETAIL_FIELDS}\n",
        "\n",
        "\n",
        "def extract_main_page_summary(page_url):\n",
        "    response = requests.get(page_url, headers=HEADERS, timeout=10)\n",
        "    time.sleep(0.2)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    container = soup.find(\"div\", class_=\"table-responsive\")\n",
        "    rows = container.find_all(\"tr\")\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for row in rows:\n",
        "        try:\n",
        "            cells = row.find_all(\"td\")\n",
        "            if len(cells) < 6:\n",
        "                continue\n",
        "\n",
        "            name = cells[0].get_text(strip=True)\n",
        "            blank = cells[1].get_text(strip=True)\n",
        "            site_url = cells[2].get_text(strip=True)\n",
        "            authority_elem = cells[3].find(\"a\")\n",
        "            authority_link = authority_elem[\"href\"] if authority_elem else None\n",
        "            authority_text = authority_elem.get_text(strip=True) if authority_elem else None\n",
        "            date = cells[4].get_text(strip=True)\n",
        "\n",
        "            btn = cells[5].find(\"button\", onclick=True)\n",
        "            btn_id = None\n",
        "            if btn:\n",
        "                onclick = btn[\"onclick\"]\n",
        "                m = re.search(r\"id=(\\d+)\", onclick)\n",
        "                if m:\n",
        "                    btn_id = m.group(1)\n",
        "\n",
        "            summary = {\n",
        "                \"Name_company\": name,\n",
        "                \"Corporate Name\": blank,\n",
        "                \"site_url\": site_url,\n",
        "                \"Authority_Name\": authority_text,\n",
        "                \"Authority_link\": authority_link,\n",
        "                \"date\": date,\n",
        "                \"id\": btn_id\n",
        "            }\n",
        "\n",
        "            detail = extract_detail_page(btn_id) if btn_id else {k: None for k in DETAIL_FIELDS}\n",
        "            full_record = {**summary, **detail}\n",
        "            all_data.append(full_record)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to parse row: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# ðŸš€ Run Script\n",
        "if __name__ == \"__main__\":\n",
        "    start_page = load_checkpoint()\n",
        "    end_page = start_page + 19  # Scrape 20 pages total\n",
        "\n",
        "    for page_number in range(start_page, end_page + 1):\n",
        "        print(f\"\\n Scraping page {page_number}...\")\n",
        "        page_url = f\"{BASE_URL}?SUBSECTION=main&page={page_number}\"\n",
        "        data = extract_main_page_summary(page_url)\n",
        "\n",
        "        final_fields = SUMMARY_FIELDS + DETAIL_FIELDS\n",
        "        output_file = f\"combined_output_page{page_number}.csv\"\n",
        "\n",
        "        with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=final_fields)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(data)\n",
        "\n",
        "        print(f\"âœ… Page {page_number} done. {len(data)} records saved to {output_file}\")\n",
        "        save_checkpoint(page_number + 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ZDnOWngS6Yn4",
        "outputId": "361037dd-2559-4526-e816-767e7e8bfc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Scraping page 21...\n",
            "[Retry 1] Detail fetch failed for ID 39383: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39382: HTTP 403\n",
            "[Retry 1] Detail fetch failed for ID 39381: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39379: HTTP 429\n",
            "[Retry 2] Detail fetch failed for ID 39379: HTTP 403\n",
            "[Retry 1] Detail fetch failed for ID 39375: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39373: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39370: HTTP 429\n",
            "[Retry 2] Detail fetch failed for ID 39370: HTTP 403\n",
            "âœ… Page 21 done. 20 records saved to combined_output_page21.csv\n",
            "\n",
            " Scraping page 22...\n",
            "[Retry 1] Detail fetch failed for ID 39368: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39366: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39364: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39362: HTTP 403\n",
            "[Retry 1] Detail fetch failed for ID 39361: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39358: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39357: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39355: HTTP 403\n",
            "[Retry 1] Detail fetch failed for ID 39352: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39350: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39349: HTTP 403\n",
            "âœ… Page 22 done. 20 records saved to combined_output_page22.csv\n",
            "\n",
            " Scraping page 23...\n",
            "[Retry 1] Detail fetch failed for ID 39348: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39346: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39344: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39342: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39340: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39338: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39336: HTTP 403\n",
            "[Retry 1] Detail fetch failed for ID 39335: HTTP 429\n",
            "[Retry 1] Detail fetch failed for ID 39334: HTTP 403\n",
            "[Retry 1] Detail fetch failed for ID 39332: HTTP 403\n",
            "[Retry 1] Detail fetch failed for ID 39330: HTTP 429\n",
            "âœ… Page 23 done. 20 records saved to combined_output_page23.csv\n",
            "\n",
            " Scraping page 24...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'find_all'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3167382065.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n Scraping page {page_number}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mpage_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{BASE_URL}?SUBSECTION=main&page={page_number}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_main_page_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mfinal_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSUMMARY_FIELDS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDETAIL_FIELDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3167382065.py\u001b[0m in \u001b[0;36mextract_main_page_summary\u001b[0;34m(page_url)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"table-responsive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir=os.listdir(\"/content\")"
      ],
      "metadata": {
        "id": "Udcl2rL4ENYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir.sort()"
      ],
      "metadata": {
        "id": "flxZ8glUETDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdir=dir[3:len(dir)-1]"
      ],
      "metadata": {
        "id": "uOBhAnlSEVWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdir.sort()"
      ],
      "metadata": {
        "id": "hGjc1tKlEhy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCJnNDALEjnx",
        "outputId": "da4b8624-a121-426b-ec68-22f8b73ff307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['combined_output_page1.csv',\n",
              " 'combined_output_page10.csv',\n",
              " 'combined_output_page11.csv',\n",
              " 'combined_output_page12.csv',\n",
              " 'combined_output_page13.csv',\n",
              " 'combined_output_page14.csv',\n",
              " 'combined_output_page15.csv',\n",
              " 'combined_output_page16.csv',\n",
              " 'combined_output_page17.csv',\n",
              " 'combined_output_page18.csv',\n",
              " 'combined_output_page19.csv',\n",
              " 'combined_output_page2.csv',\n",
              " 'combined_output_page20.csv',\n",
              " 'combined_output_page21.csv',\n",
              " 'combined_output_page22.csv',\n",
              " 'combined_output_page23.csv',\n",
              " 'combined_output_page3.csv',\n",
              " 'combined_output_page4.csv',\n",
              " 'combined_output_page5.csv',\n",
              " 'combined_output_page6.csv',\n",
              " 'combined_output_page7.csv',\n",
              " 'combined_output_page8.csv',\n",
              " 'combined_output_page9.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Step 1: Zip all CSV files in the current directory\n",
        "zip_filename = \"all_csvs.zip\"\n",
        "with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
        "    for filename in os.listdir():\n",
        "        if filename.endswith(\".csv\"):\n",
        "            zipf.write(filename)\n",
        "\n",
        "# Step 2: Download the zip\n",
        "files.download(zip_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BtKOzxipFJ6s",
        "outputId": "4dd09e95-8135-43d6-db97-f629b3ef2ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f2489a18-8d1d-4cfe-a95d-4f0cc74370c8\", \"all_csvs.zip\", 252571)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Step 1: List all CSV files\n",
        "csv_files = [f for f in os.listdir() if f.endswith(\".csv\") and f.startswith(\"combined_output_page\")]\n",
        "\n",
        "# Step 2: Sort filenames to keep pages in order\n",
        "csv_files.sort(key=lambda x: int(x.split(\"page\")[1].split(\".\")[0]))\n",
        "\n",
        "# Step 3: Read and append all CSVs\n",
        "combined_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "\n",
        "# Step 4: Save to a final combined file\n",
        "combined_df.to_csv(\"iosco_combined.csv\", index=False)\n",
        "\n",
        "print(f\"âœ… Combined {len(csv_files)} CSVs into 'iosco_combined.csv' with {len(combined_df)} rows.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrEm9tYPFMUZ",
        "outputId": "eddea464-6d31-42ec-8f12-56180555d704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Combined 23 CSVs into 'iosco_combined.csv' with 460 rows.\n"
          ]
        }
      ]
    }
  ]
}